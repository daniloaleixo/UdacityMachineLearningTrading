{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q-learning Implementation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPU4FUsNAd8fNhSBvczcNEi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daniloaleixo/UdacityMachineLearningTrading/blob/master/Q_learning_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEqzVr8u8_Gc",
        "colab_type": "text"
      },
      "source": [
        "# Q Learning Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgMJlSE_86qB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5553130c-9cbe-48c2-9fa4-dc4cf03cf2ad"
      },
      "source": [
        "!pip install cmake 'gym[atari]' scipy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cmake in /usr/local/lib/python3.6/dist-packages (3.12.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.15.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.2.2)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.4.10)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.17.5)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (6.2.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA81ylyq9Kol",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "92a9add5-2856-433b-a82e-716a94828af5"
      },
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"Taxi-v3\").env\n",
        "\n",
        "env.render()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juVKoCQp9yvv",
        "colab_type": "text"
      },
      "source": [
        "## The problem\n",
        "\n",
        "Here's our restructured problem statement (from Gym docs):\n",
        "\n",
        "\"There are 4 locations (labeled by different letters), and our job is to pick up the passenger at one location and drop him off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzLutUu99Y6U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "26c3b638-28cd-4134-d80e-8144646e2de0"
      },
      "source": [
        "env.reset() # reset environment to a new, random state\n",
        "env.render()\n",
        "\n",
        "print(\"Action Space {}\".format(env.action_space))\n",
        "print(\"State Space {}\".format(env.observation_space))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[43m \u001b[0m|\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7JH4I3O9_-1",
        "colab_type": "text"
      },
      "source": [
        "- The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
        "- The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
        "- R, G, Y, B are the possible pickup and destination locations. The blue letter represents the current passenger pick-up location, and the purple letter is the current destination."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1tW-dLu-aBC",
        "colab_type": "text"
      },
      "source": [
        "As verified by the prints, we have an Action Space of size 6 and a State Space of size 500. As you'll see, our RL algorithm won't need any more information than these two things. All we need is a way to identify a state uniquely by assigning a unique number to every possible state, and RL learns to choose an action number from 0-5 where:\n",
        "\n",
        "- 0 = south\n",
        "- 1 = north\n",
        "- 2 = east\n",
        "- 3 = west\n",
        "- 4 = pickup\n",
        "- 5 = dropoff\n",
        "\n",
        "Recall that the 500 states correspond to a encoding of the taxi's location, the passenger's location, and the destination location.\n",
        "\n",
        "Reinforcement Learning will learn a mapping of states to the optimal action to perform in that state by exploration, i.e. the agent explores the environment and takes actions based off rewards defined in the environment.\n",
        "\n",
        "The optimal action for each state is the action that has the highest cumulative long-term reward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xnrF158-8Jm",
        "colab_type": "text"
      },
      "source": [
        "## State map\n",
        "We have a map of state from each index to an integer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1-_hs5r9376",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "13db8ccc-7568-4b00-f0ea-926e043cb66b"
      },
      "source": [
        "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
        "print(\"State:\", state)\n",
        "\n",
        "env.s = state\n",
        "env.render()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State: 328\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6huTjxzI_KfA",
        "colab_type": "text"
      },
      "source": [
        "## The Reward Table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2HJC0ID_Vxa",
        "colab_type": "text"
      },
      "source": [
        "When the Taxi environment is created, there is an initial Reward table that's also created, called `P`. We can think of it like a matrix that has the number of states as rows and number of actions as columns, i.e. a states Ã— actions matrix.\n",
        "\n",
        "Since every state is in this matrix, we can see the default reward values assigned to our illustration's state:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfBJkvOs-s3a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "eb463938-fb4a-49e4-ff73-29e7d455bf68"
      },
      "source": [
        "env.P[328]\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 428, -1, False)],\n",
              " 1: [(1.0, 228, -1, False)],\n",
              " 2: [(1.0, 348, -1, False)],\n",
              " 3: [(1.0, 328, -1, False)],\n",
              " 4: [(1.0, 328, -10, False)],\n",
              " 5: [(1.0, 328, -10, False)]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atmw3a1X_nwT",
        "colab_type": "text"
      },
      "source": [
        "This dictionary has the structure \n",
        "\n",
        "```\n",
        "{action: [(probability, nextstate, reward, done)]}\n",
        "```\n",
        "\n",
        "\n",
        "A few things to note:\n",
        "\n",
        "- The 0-5 corresponds to the actions (south, north, east, west, pickup, dropoff) the taxi can perform at our current state in the illustration.\n",
        "- In this env, probability is always 1.0.\n",
        "- The nextstate is the state we would be in if we take the action at this index of the dict\n",
        "- All the movement actions have a -1 reward and the pickup/dropoff actions have -10 reward in this particular state. If we are in a state where the taxi has a passenger and is on top of the right destination, we would see a reward of 20 at the dropoff action (5)\n",
        "- *done* is used to tell us when we have successfully dropped off a passenger in the right location. Each successfull dropoff is the end of an episode\n",
        "\n",
        "Note that if our agent chose to explore action two (2) in this state it would be going East into a wall. The source code has made it impossible to actually move the taxi across a wall, so if the taxi chooses that action, it will just keep accruing -1 penalties, which affects the long-term reward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsCyCuvdAoI4",
        "colab_type": "text"
      },
      "source": [
        "## Solving the environment without Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHqW1USrAx30",
        "colab_type": "text"
      },
      "source": [
        "Let's see what would happen if we try to brute-force our way to solving the problem without RL.\n",
        "\n",
        "Since we have our P table for default rewards in each state, we can try to have our taxi navigate just using that.\n",
        "\n",
        "We'll create an infinite loop which runs until one passenger reaches one destination (one episode), or in other words, when the received reward is 20. The env.action_space.sample() method automatically selects one random action from set of all possible actions.\n",
        "\n",
        "Let's see what happens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqAewhcb_XrJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3ac1c9f1-a56a-4b40-d63c-a5a31eea58d6"
      },
      "source": [
        "env.s = 328  # set environment to illustration's state\n",
        "\n",
        "epochs = 0\n",
        "penalties, reward = 0, 0\n",
        "\n",
        "frames = [] # for animation\n",
        "\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, done, info = env.step(action)\n",
        "\n",
        "    if reward == -10:\n",
        "        penalties += 1\n",
        "    \n",
        "    # Put each rendered frame into dict for animation\n",
        "    frames.append({\n",
        "        'frame': env.render(mode='ansi'),\n",
        "        'state': state,\n",
        "        'action': action,\n",
        "        'reward': reward\n",
        "        }\n",
        "    )\n",
        "\n",
        "    epochs += 1\n",
        "    \n",
        "    \n",
        "print(\"Timesteps taken: {}\".format(epochs))\n",
        "print(\"Penalties incurred: {}\".format(penalties))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Timesteps taken: 222\n",
            "Penalties incurred: 78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4pycBkwBCwM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "b85d538f-7774-4a32-87c8-00a369895ea2"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.1)\n",
        "        \n",
        "print_frames(frames)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Timestep: 1559\n",
            "State: 0\n",
            "Action: 5\n",
            "Reward: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ws9OJdGBgQ5",
        "colab_type": "text"
      },
      "source": [
        "Not good. Our agent takes thousands of timesteps and makes lots of wrong drop offs to deliver just one passenger to the right destination.\n",
        "\n",
        "This is because we aren't learning from past experience. We can run this over and over, and it will never optimize. The agent has no memory of which action was best for each state, which is exactly what Reinforcement Learning will do for us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5au6VDbB4q2",
        "colab_type": "text"
      },
      "source": [
        "## Enter Reinforcement Learning\n",
        "\n",
        "We are going to use a simple RL algorithm called Q-learning which will give our agent some memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dMww4mcB-o4",
        "colab_type": "text"
      },
      "source": [
        "### Implementing Q-learning in python\n",
        "#### Training the Agent\n",
        "First, we'll initialize the Q-table to a 500Ã—6 matrix of zeros:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXBJxD5nBHGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlZNEq68LoZF",
        "colab_type": "text"
      },
      "source": [
        "We can now create the training algorithm that will update this Q-table as the agent explores the environment over thousands of episodes.\n",
        "\n",
        "In the first part of while not done, we decide whether to pick a random action or to exploit the already computed Q-values. This is done simply by using the epsilon value and comparing it to the random.uniform(0, 1) function, which returns an arbitrary number between 0 and 1.\n",
        "\n",
        "We execute the chosen action in the environment to obtain the next_state and the reward from performing the action. After that, we calculate the maximum Q-value for the actions corresponding to the next_state, and with that, we can easily update our Q-value to the new_q_value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXRjS6PdLVev",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "3ffacc06-a7d3-413b-f493-fb41a91e2414"
      },
      "source": [
        "%%time\n",
        "\"\"\"Training the agent\"\"\"\n",
        "\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1\n",
        "\n",
        "# For plotting metrics\n",
        "all_epochs = []\n",
        "all_penalties = []\n",
        "\n",
        "for i in range(1, 100001):\n",
        "    state = env.reset()\n",
        "\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample() # Explore action space\n",
        "        else:\n",
        "            action = np.argmax(q_table[state]) # Exploit learned values\n",
        "\n",
        "        next_state, reward, done, info = env.step(action) \n",
        "        \n",
        "        old_value = q_table[state, action]\n",
        "        next_max = np.max(q_table[next_state])\n",
        "        \n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "        \n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episode: {i}\")\n",
        "\n",
        "print(\"Training finished.\\n\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n",
            "CPU times: user 57 s, sys: 6.01 s, total: 1min 2s\n",
            "Wall time: 58.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Tt1UprqMX4P",
        "colab_type": "text"
      },
      "source": [
        "Now that the Q-table has been established over 100,000 episodes, let's see what the Q-values are at our illustration's state:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKY7knYyLtao",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c1620fa2-4a35-4f58-91b0-f8bbdf99dacf"
      },
      "source": [
        "q_table[328]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -2.40254532,  -2.27325184,  -2.41029933,  -2.3615079 ,\n",
              "       -11.05888207, -11.13904531])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDwUOCr5Md-x",
        "colab_type": "text"
      },
      "source": [
        "The max Q-value is \"north\" (-1.971), so it looks like Q-learning has effectively learned the best action to take in our illustration's state!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxL-TOjjMi1d",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluating the agent\n",
        "\n",
        "Let's evaluate the performance of our agent. We don't need to explore actions any further, so now the next action is always selected using the best Q-value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WC75x9rMjWi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2f6dc9d5-64f4-47d1-a2f1-d66636a9876c"
      },
      "source": [
        "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
        "\n",
        "total_epochs, total_penalties = 0, 0\n",
        "episodes = 100\n",
        "\n",
        "for _ in range(episodes):\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state])\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        epochs += 1\n",
        "\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 12.6\n",
            "Average penalties per episode: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUVg2sxQMvsG",
        "colab_type": "text"
      },
      "source": [
        "We can see from the evaluation, the agent's performance improved significantly and it incurred no penalties, which means it performed the correct pickup/dropoff actions with 100 different passengers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQFkFIinMobv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}