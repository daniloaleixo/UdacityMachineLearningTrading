{
 "cells": [
    {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "### <font color=blue>Reinforcement learning</font>\n",
    "![ML](images/ml_16.png) \n",
    "![ML](images/ml_15.png) \n",
    "#### Markov Decision Problems\n",
    "* Set of states **`S`** \n",
    "* Set of actions **`A`**\n",
    "* Transition function **`T[s,a,s']`** \n",
    "* Reward function **`R[s,a]`**\n",
    "* Find **`policy π(s)`** that will maximize reward over time\n",
    "* If we have T and R, there are algorithms we can unleash that will find this optimal policy. Two of them are policy iteration and value iteration.\n",
    "\n",
    "\n",
    "#### Defined by:\n",
    "* A Markov decision problem is defined by S, A, T and R. \n",
    "* S is the potential states. \n",
    "* A are the potential actions. \n",
    "* T is a transition probability, which is given the state S we're in, the action A we're taking, and ends up in state S'. \n",
    "* R is the reward function. \n",
    "* The goal for reinforcement learning algorithm is to find a policy, π, that maps a state to an action that we should take, and its goal is to find this π such that it maxismizes some future sum of the reward.\n",
    "\n",
    "#### Two approaches to find policy π \n",
    "* Model-based  \n",
    " * Build models of T[s,a,s'] and R[s,a] to solve problems using value iteration or policy iteration. \n",
    "* Model-free  \n",
    " * Q-Learning\n",
    " \n",
    "#### What to optimize ?\n",
    "![ML](images/ml_17.png) \n",
    "$\\lambda$ relates very strongly to interest rates.\n",
    "If $\\lambda$  were 0.95, it means each step in the future is worth about 5% less than the immediate reward if we got it right away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "### <font color=blue>Q-learning</font>\n",
    "![ML](images/ml_18.png) \n",
    "We can think of it as a table which gets two-dimensions, $s$ and $a$. <br/>\n",
    "$s$ is the state we're looking at. <br/>\n",
    "$a$ is the action we might take. Q represents the value of taking action $a$ in state $s$.<br/>\n",
    "Two components: immediate reward that we get for taking action $a$ in state $s$  plus discounted reward which is the reward we get for future actions. Q represents the rewards we get for acting now and in the future.<br/><br/>\n",
    "$\\pi(s)$ represents the policy which means, what is the action we take when we are in state $s$ or what is the policy for state $s$? We take advantage of our Q table to figure that out.<br/>\n",
    "We're in state $s*$ and we want to find out which action is the best. All we need to do is look across all the potential actions and find out which value of $Q[s,a]$ is maximized. We don't change $s$, just step through each value of $a$ and the one that is the largest is the action we should take.<br/>\n",
    "![ML](images/ml_19.png) \n",
    "\n",
    "#### How does it take that information to improve this Q table? \n",
    "![ML](images/ml_20.png) \n",
    "There are two main parts to the update rule. \n",
    "* A low value of alpha, for instance, means that in this update rule, the previous value for Q of s,a is more stronly preserved. \n",
    "* A low value of gamma means that we value later rewards less, which equates to essentially a high discount rate. \n",
    "* A high value of gamma means that we value later rewards very significantly.\n",
    "* $a'$ is the next action we will take. \n",
    "* $argmax_{a'}(Q[s', a'])$ means that we will find the best action $a'$ that maximizes the value when we're in that state $s'$.\n",
    "\n",
    "#### Update Rule\n",
    "The formula for computing $Q$ for any state-action pair $<s, a>$, given an experience tuple $<s, a, s', r>$, is:\n",
    "$Q'[s, a] = (1 - α) · Q[s, a] + α · (r + γ · Q[s', argmaxa'(Q[s', a'])])$\n",
    "\n",
    "Here:\n",
    "\n",
    "* $r = R[s, a]$ is the immediate reward for taking action $a$ in state $s$,\n",
    "* $γ ∈ [0, 1]$ (gamma) is the discount factor used to progressively reduce the value of future rewards,\n",
    "* $s'$ is the resulting next state,\n",
    "* $argmaxa'(Q[s', a'])$ is the action that maximizes the Q-value among all possible actions $a'$ from $s'$, and,\n",
    "* $α ∈ [0, 1]$ (alpha) is the learning rate used to vary the weight given to new experiences compared with past Q-values.\n",
    "\n",
    "#### Two Finer Points\n",
    "![ML](images/ml_21.png) \n",
    "* choose random action with probability C  \n",
    "* Pick the actio with the highest Q value\n",
    "\n",
    "#### The Trading Problem: Actions\n",
    "![ML](images/ml_22.png) \n",
    "Three actions:\n",
    "* Buy\n",
    "* Sell\n",
    "* Do nothing(Hold)\n",
    "\n",
    "#### The Trading Problem: Rewards\n",
    "Rewards should relate in some way to the returns of our strategy\n",
    "* Short-term rewards in terms of daily returns  \n",
    "* Long-term rewards that reflect the cumulative return of a trade cycle from a buy to a sell or for shorting from a sell to a buy\n",
    "For faster convergence:\n",
    "* **`r=daily return`**:  this one is called an immediate reward which is faster to converge,. \n",
    "If we reward a little bit on each day, the learner is able to learn much more quickly because it gets much more frequent rewards.\n",
    "* **`r=0 until exit, then cumulative return`**: this one is called delayed reward. If we use this one, we get no rewards at all until the end of a trade cycle, from a buy to a sell. The learner has to infer from that final reward all the way back that each action in sequence there must have been accomplished in the right order to get the reward.\n",
    "\n",
    "#### The Trading Problem: State\n",
    "[ ]**`Adjusted close`**: cannot be able to generalize over different price regimes for when the stock was low to when it was high. If we're trying to learn a model for several stocks at once and they each hold very different prices, adjusted close doesn't serve well to help us generalize.<br/>\n",
    "[ ]**`Simple Moving Average (SMA)`**<br/>\n",
    "[v]**`Adjusted close / SMA`**: combine Adjust close and SMA together into a ratio that makes a good factor to use in state.<br/>\n",
    "[v]**`Bollinger Band value`**<br/>\n",
    "[v]**`P/E ratio`**<br/>\n",
    "[v]**`Holding stock`**<br/>\n",
    "[v]**`Return since entry`**: the return since we enter the position. This might help us set exit points,for instance, maybe we've made 10% on the stock since we bought it and we should take our winnings while we can.<br/>\n",
    "\n",
    "#### Creating the state\n",
    "![ML](images/ml_23.png) \n",
    "* State ia an integer\n",
    "* Discretize each factor which essentially means to convert the real number into an integer.\n",
    "* Combine: combine all of those integers together into a single number.<br/>\n",
    "\n",
    "Steps:<br/>\n",
    " * Assuming we're using a discrete state space that means more or less that our overall state is going to be this one integer that represents at once all of our factors.\n",
    " * Consider we have 4 factors and each one is a real number.\n",
    " * Run each of these factors through their individual discretizers and we get an integer.\n",
    " * Then we've happened to select integers between 0 and 9, but we can have larger ranges, for isntance, 0 to 20 or 0 to 100 even.\n",
    " * Stack them one after the other into our overall discretized state.\n",
    " \n",
    "#### Discretizing\n",
    "![ML](images/ml_24.png) \n",
    "![ML](images/ml_25.png) \n",
    "Use a way to convert a real number into an integer across a limited scale. In other words, we might have hundres of individual values here between 0 and 25 of a real number. We want to convert that into an integer say between 0 and 9.\n",
    "* First thing is we determine ahead of time how many steps we're going to have. In other words, how many groups do we want to be able to put the data into?\n",
    "* So we divide how mnay data elements we have all together by the number of steps.\n",
    "* Then we sort the data and then the threshold just end up being the locations for each one of these values. In other words, if we had, say, 100 data elements, 10 steps, then our step size is 10. So we just find the 10th data element which is our first threshold and then 20th and 30th and so on.\n",
    "* The threshold might end up looking something like the figure.\n",
    "When we go to query and have a new value between those two threshold, 7 and 8, we'll see the value wwould be an 8.\n",
    "\n",
    "#### Summary\n",
    "**`Advantages`**<br/>\n",
    "The main advantage of a model-free approach like Q-Learning over model-based techniques is that it can easily be applied to domains where all states and/or transitions are not fully defined.\n",
    "As a result, we do not need additional data structures to store transitions T(s, a, s') or rewards R(s, a).\n",
    "Also, the Q-value for any state-action pair takes into account future rewards. Thus, it encodes both the best possible value of a state (maxa Q(s, a)) as well as the best policy in terms of the action that should be taken (argmaxa Q(s, a)).<br/><br/>\n",
    "**`Issues`**<br/>\n",
    "The biggest challenge is that the reward (e.g. for buying a stock) often comes in the future - representing that properly requires look-ahead and careful weighting.\n",
    "Another problem is that taking random actions (such as trades) just to learn a good strategy is not really feasible (you'll end up losing a lot of money!).\n",
    "In the next lesson, we will discuss an algorithm that tries to address this second problem by simulating the effect of actions based on historical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### <font color=blue>Dyna</font>\n",
    "One problem with Q-learning is that it takes many experienced tuples to converge. This is expensive in terms of interacting with the world because we have to take a real step, in other words, execute a trade, in order to gather information. \n",
    "<br/><br/>\n",
    "Dyna works by building models of T, the transition matrix, and R, the reward matrix. Then after each real interactin with the world, we hallucinate many additional interactions, usually a few hundred. That are used then to update the Q table.\n",
    "<br/><br/>\n",
    "Dyna is intended to speed up learning or model convergence for Q-learning.\n",
    "\n",
    "Q-learning is a model-free which means that it does not rely on T (transition matrix) or R (reward function). Q-learning does not know both of them.\n",
    "\n",
    "Dyna ends up becoming a blend of model-free and model-based methods.\n",
    "\n",
    "![ML](images/ml_26.png) \n",
    "* Initializa the Q table, and begin iterating \n",
    "* Observe S\n",
    "* Execute action A, and then observe new state, S', and reward,R\n",
    "* Update Q table with this experience tuple and repeat.<br/>\n",
    "<br/>\n",
    "<b>When we augment Q learning with Dyna-Q, we had 3 new components:</b>\n",
    "* Learn Model: add some logic that enables us to learn models of T and R\n",
    "* Halucinate experience: rather than interacting with the real world like we do appear with Q learning part and this is expensive by the way.\n",
    "We halucinate these experiences, update our Q table.\n",
    "* Update Q: updated by 2. and repeat many times like 100s times.\n",
    "\n",
    "So we can leverage the experience we gain in Q-learning from an interaction with the real world, but then update our model in Dyna-Q more completely before we step out and interact with the real world again.\n",
    "\n",
    "After we've interated enough times in Dyna-Q, then we return back up to Q-learning and resume our interaction with the real world.\n",
    "\n",
    "The key thing is that for each experience with the real world, we have maybe 100 or 200 updates of our model in Dyna-Q.\n",
    "![ML](images/ml_27.png) \n",
    "Then we find new values for T and R.\n",
    "the point where we update includes the following:\n",
    "* We want to update T, called T' here, which represents our transition matrix and update our reward function, called R'.\n",
    "T' is the probability that if we are in state s and we take aciton a, it will end up in s'.\n",
    "R' is our expected reward if we are in state s and we take action a.\n",
    "\n",
    "How to update T' and R'?\n",
    "1. randomly select an s.\n",
    "2. randomly select an a.\n",
    "3. Infer our new state s' by looking at T.\n",
    "4. Infer a reward, our immediate reward r by looking at big R or R table.\n",
    "\n",
    "Now we've got s, a, s', r or a complete experience tuple and we can update our Q-table using that.\n",
    "\n",
    "Q table update is our final step.\n",
    "\n",
    "####  <font color=red>Learning T</font>\n",
    "![ML](images/ml_28.png) \n",
    "T(s, a, s') represents the probability that if we are in state s, take action a, we will end up in state s'.\n",
    "\n",
    "To learn the model of T, we just observe how these transitions occur, in other words, we'll have experience with the real world, we'll get back on s, a, s', we just count how many times it did happened.\n",
    "\n",
    "We introduce new table called $T_{count}$ or $T_c$\n",
    "1. initialize all of our T count values to be a very small number.\n",
    "2. begin executing Q learning. each time we interact with real world we observe, s, a, and s'.\n",
    "3. increment that location in our $T_{count}$ matrix.<br/>\n",
    "\n",
    "####  <font color=red>Evaluating T</font>\n",
    "![ML](images/ml_29.png) \n",
    "sum over i where we have i iterate over all the possible states of T[s, a, , i]. This is the number of times in total that we're in state s and executed action a.\n",
    "\n",
    "#### <font color=red>Learning R</font>\n",
    "![ML](images/ml_30.png) \n",
    "<font color=blue>$R[s,a]$</font> is a model that is expected reward if we're in state s and execute action a.\n",
    "<font color=blue>$r$</font> is our immediate reward when we experience this in the real world, in other words, it's what we get in an experience tuple.\n",
    "\n",
    "So we want to update this model every time we have a real experience.\n",
    "\n",
    "Similar to Q-table update equation:\n",
    "<font color=blue>$\\alpha$</font>: learning rate\n",
    "<font color=blue>$r$</font>: new best estimate or immediate reward of what value should be.\n",
    "\n",
    "we're waiting presumably, our old value more than our new value\n",
    "\n",
    "#### <font color=red>Dyna-Q recap</font>\n",
    "![ML](images/ml_31.png) \n",
    "\n",
    "#### <font color=red>Summary</font>\n",
    "![ML](images/ml_32.png) \n",
    "The Dyna architecture consists of a combination of:\n",
    "* direct reinforcement learning from real experience tuples gathered by acting in an environment,\n",
    "* updating an internal model of the environment, and,\n",
    "* using the model to simulate experiences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
